import asyncio
import json
import os
import re
from datetime import datetime
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from bs4 import BeautifulSoup


class NurContentExtractor:
    """é’ˆå¯¹ nur.cn ç½‘ç«™çš„å†…å®¹æå–å™¨"""
    
    def __init__(self, output_dir):
        self.dataset = []
        self.output_dir = output_dir
        self.visited_urls = set()
        os.makedirs(output_dir, exist_ok=True)
        
    def is_duplicate(self, url):
        """æ£€æŸ¥URLæ˜¯å¦å·²å¤„ç†è¿‡"""
        if url in self.visited_urls:
            return True
        self.visited_urls.add(url)
        return False
        
    def extract_content(self, html_content, url):
        """ä»HTMLä¸­æå– class="tt" (æ ‡é¢˜) å’Œ class="view_p mazmun" (æ­£æ–‡) çš„çº¯æ–‡æœ¬"""
        if not html_content:
            return None
            
        soup = BeautifulSoup(html_content, 'lxml')
        
        title_elem = soup.find('h2', class_='tt')
        title = title_elem.get_text(strip=True) if title_elem else ""
        
        content_elem = soup.find('div', class_='view_p mazmun')
        content = ""
        if content_elem:
            content = content_elem.get_text(separator='\n', strip=True)
        
        if not title and not content:
            return None
            
        return {
            "url": url,
            "title": title,
            "content": content,
            "crawl_time": datetime.now().isoformat(),
            "content_length": len(content)
        }
    
    def save_single_txt(self, data):
        """ä¿å­˜å•ä¸ªæ–‡ç« ä¸º txt æ–‡ä»¶"""
        if not data['title']:
            return None
        
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', data['title'])[:50]
        txt_filename = f"{safe_title}.txt"
        txt_path = os.path.join(self.output_dir, txt_filename)
        
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"{data['title']}\n\n")
            f.write(data['content'])
        
        return txt_filename
    
    def load_existing_urls(self):
        """åŠ è½½å·²å­˜åœ¨çš„URLå»é‡"""
        for f in os.listdir(self.output_dir):
            if f.endswith('.json'):
                try:
                    with open(os.path.join(self.output_dir, f), 'r', encoding='utf-8') as fp:
                        data = json.load(fp)
                        if 'articles' in data:
                            for article in data['articles']:
                                self.visited_urls.add(article['url'])
                except:
                    pass


async def deep_crawl_nur(max_pages=50000, max_depth=3):
    """æ·±åº¦çˆ¬å– nur.cn çš„æ–°é—»é¡µé¢
    
    Args:
        max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•° (é»˜è®¤ 50000)
        max_depth: æœ€å¤§çˆ¬å–æ·±åº¦ (é»˜è®¤ 3)
    """
    
    output_dir = "nur_articles"
    extractor = NurContentExtractor(output_dir)
    extractor.load_existing_urls()
    
    print(f"ğŸ“‹ å·²å­˜åœ¨ {len(extractor.visited_urls)} æ¡è®°å½•ï¼Œå°†è·³è¿‡é‡å¤å†…å®¹")
    
    deep_strategy = BFSDeepCrawlStrategy(
        max_depth=max_depth,
        include_external=False,
        max_pages=max_pages,
    )
    
    config = CrawlerRunConfig(
        deep_crawl_strategy=deep_strategy,
        scraping_strategy=LXMLWebScrapingStrategy(),
        cache_mode=CacheMode.BYPASS,
        stream=True,
        verbose=True,
    )
    
    print(f"ğŸš€ å¼€å§‹æ·±åº¦çˆ¬å– https://www.nur.cn/")
    print(f"ğŸ“Š é…ç½®: æœ€å¤§é¡µé¢={max_pages}, æœ€å¤§æ·±åº¦={max_depth}")
    print(f"ğŸ“ æ–‡ç« å°†ä¿å­˜åˆ°: {output_dir}/")
    print("=" * 60)
    
    async with AsyncWebCrawler() as crawler:
        result_count = 0
        success_count = 0
        skip_count = 0
        
        async for result in await crawler.arun("https://www.nur.cn/", config=config):
            result_count += 1
            
            if result.success and result.html:
                if '/news/' in result.url and result.url.endswith('.shtml'):
                    if extractor.is_duplicate(result.url):
                        skip_count += 1
                        continue
                        
                    data = extractor.extract_content(result.html, result.url)
                    
                    if data:
                        extractor.dataset.append(data)
                        success_count += 1
                        
                        txt_file = extractor.save_single_txt(data)
                        
                        if success_count % 100 == 0:
                            print(f"âœ… å·²ä¿å­˜ {success_count} ç¯‡: {data['title'][:30]}...")
            
            if result_count % 500 == 0:
                print(f"   ğŸ“Š è¿›åº¦: {result_count} é¡µé¢, {success_count} æˆåŠŸ, {skip_count} è·³è¿‡")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š çˆ¬å–å®Œæˆ!")
    print(f"   - æ€»é¡µé¢æ•°: {result_count}")
    print(f"   - æˆåŠŸæå–: {success_count}")
    print(f"   - è·³è¿‡é‡å¤: {skip_count}")
    
    output_file = save_final_dataset(extractor.dataset, output_dir)
    print(f"\nğŸ’¾ æ•°æ®é›†å·²ä¿å­˜è‡³: {output_file}")
    
    return extractor.dataset


def save_final_dataset(dataset, output_dir):
    """ä¿å­˜æœ€ç»ˆæ•°æ®é›†åˆ°æ–‡ä»¶"""
    if not dataset:
        print("âš ï¸ æ— æ–°æ•°æ®éœ€è¦ä¿å­˜")
        return None
        
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    json_file = os.path.join(output_dir, f"nur_dataset_{timestamp}.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            "source": "https://www.nur.cn/",
            "crawl_date": datetime.now().isoformat(),
            "total_articles": len(dataset),
            "articles": dataset
        }, f, ensure_ascii=False, indent=2)
    
    return json_file


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='çˆ¬å– nur.cn æ–°é—»å†…å®¹')
    parser.add_argument('--pages', type=int, default=50000, help='æœ€å¤§çˆ¬å–é¡µé¢æ•° (é»˜è®¤ 50000)')
    parser.add_argument('--depth', type=int, default=3, help='æœ€å¤§çˆ¬å–æ·±åº¦ (é»˜è®¤ 3)')
    
    args = parser.parse_args()
    
    dataset = asyncio.run(deep_crawl_nur(max_pages=args.pages, max_depth=args.depth))
